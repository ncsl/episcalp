{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Spike Rates From Lay Files\n",
    "\n",
    "MNE has some useful features that will allow us to quickly read the spike data and calculate statistics of interest.\n",
    "\n",
    "**Note:** mne must have version 0.23 or else there is a bug in the annotation reader\n",
    "\n",
    "## Import Tools\n",
    "\n",
    "**Alana T**\n",
    "\n",
    "**NCSL**\n",
    "\n",
    "**Last updated: 8/14/2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import glob\n",
    "import mne\n",
    "%matplotlib inline\n",
    "import matplotlib as pml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.core.common import flatten\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,\n",
    "    StratifiedShuffleSplit\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    auc,\n",
    "    roc_curve,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare xlsx file for dataframe usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel file from path 'xlsdat' into a dataframe, dropping extraneous columns\n",
    "# Change file path to your data folder\n",
    "xlsdat = r\"C:\\Users\\atill\\OneDrive - Johns Hopkins\\SarmaLab\\spike_detection\\JHU_scalp_clinical_datasheet_raw.xlsx\"\n",
    "df = pd.read_excel(xlsdat)\n",
    "# Drop extraneous columns\n",
    "df.drop(df.columns[3:len(df.columns)], axis=1, inplace=True) \n",
    "df = df.loc[df.index.repeat(df.number_datasets)]\n",
    "df = df.reset_index()\n",
    "df = df.drop('number_datasets',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relabel patient_id data to correspond to patient condition (0: normal, 0.5: norm-ep, 1: abnorm-ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = lambda x : 0 if x < 100 else (0.5 if x < 200 else 1)\n",
    "df['patient_id'] = df.patient_id.apply(filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for analyzing .lay files\n",
    "\n",
    "### Run analyze_lay to find spike data by patient and spike data by patient by patient channel\n",
    "\n",
    "### input is a file path where lay files are located\n",
    "\n",
    "### output is two dictionaries containing average spike rate for a pt and the spike rate on individual channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike analysis code from Patrick: \n",
    "# Extract important information from the object (annotations, duration of recording, number of total channels)\n",
    "def analyze_lay(lay_fpath):\n",
    "    raw = mne.io.read_raw_persyst(lay_fpath)\n",
    "    verbose = True\n",
    "    annotations = raw.annotations\n",
    "    mins = raw.n_times/(60*raw.info['sfreq'])  # We will be calculating spikes/min in each channel\n",
    "    n_chs = raw.info['nchan']\n",
    "    # Retrieve spike annotations, get rid of information except channel name\n",
    "    spike_channels = [a['description'].split(' ')[1] for a in annotations if 'spike' in a['description']]\n",
    "    # Extract relevant rates\n",
    "    def count_spikes(spike_list):\n",
    "        spike_dict = {}\n",
    "        for ch in list(set(spike_list)):\n",
    "            spike_dict.update({ch: spike_list.count(ch)})\n",
    "        return spike_dict\n",
    "    spike_counts = count_spikes(spike_channels)\n",
    "    spike_rates = {}\n",
    "    for key, val in spike_counts.items():\n",
    "        spike_rates[key] = val / mins\n",
    "    total_spike_rate = sum(spike_counts.values()) / (n_chs*mins)  # normalizing by total time and number of channels\n",
    "    return [total_spike_rate, spike_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to extract important information from lay data and appent it to a dataframe\n",
    "\n",
    "### Inputs are a dictionary for label: data (a_dict, e.g. hospital_id: avg. spikes/min), a dataframe to append data to (a_df, e.g. df, with a dictionary to df mapping of 'hospital_id' (can be changed)), and a section name for that dataframe (a_secname, e.g. 'spikes')\n",
    "\n",
    "### Output is a dataframe with a new column in a_df mapped from a_dict under the column name a_secname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbidx(a_dict, a_df, a_secname):\n",
    "    dup = []\n",
    "    a_dict2 = a_dict.copy()\n",
    "    for key, value in a_dict.items():\n",
    "        for k in a_dict.keys():\n",
    "            if ((key[0:2] == k[0:2]) and (key != k)):\n",
    "                if key[0:2] not in dup:\n",
    "                    dup.append(key[0:2])\n",
    "                a_dict2[key] = [a_dict[key], a_dict[k]]\n",
    "        if '-' in key[0:2]:\n",
    "            a_dict2[int(key[0:1])] = a_dict2[key]\n",
    "            a_dict2.pop(key)\n",
    "        else:\n",
    "            a_dict2[int(key[0:2])] = a_dict2[key]\n",
    "            a_dict2.pop(key)\n",
    "    a_df[a_secname] = a_df['hospital_id'].map(a_dict2)\n",
    "    a_df2 = a_df.copy()\n",
    "    for k in dup:\n",
    "        for index in range(0,len(a_df)-1):\n",
    "            if (a_df.iloc[index].hospital_id == int(k)) and (a_df.iloc[index+1].hospital_id == int(k)):\n",
    "                val = a_df.iloc[index][a_secname]\n",
    "                a_df2.at[index, a_secname] = val[0]\n",
    "                a_df2.at[index+1, a_secname] = val[1]\n",
    "    return a_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to report TP, FP, TN, FN (performance measure)\n",
    "\n",
    "### Inputs are actual labels (y_actual0), predicted labels (y_hat0), and a theta threshold (theta)\n",
    "\n",
    "### Modifies each point from the input into 1 or 0 after comparison to a theta via .apply (e.g. 0.6 --> 1, 0.2 -->0)\n",
    "\n",
    "### Outputs are TP, FP, TN, FN counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual0, y_hat0, theta):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    binry = list()\n",
    "    y_hat = y_hat0.to_list()\n",
    "    y_actual = y_actual0.to_list()\n",
    "    \n",
    "    for i in range(len(y_hat)):\n",
    "        if y_hat[i] <= theta:\n",
    "            binry.append(0)\n",
    "        elif y_hat[i] >= theta:\n",
    "            binry.append(1)\n",
    "    \n",
    "    for i in range(len(binry)): \n",
    "        if y_actual[i]==binry[i]==1:\n",
    "           TP += 1\n",
    "        if binry[i]==1 and y_actual[i]!=binry[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==binry[i]==0:\n",
    "           TN += 1\n",
    "        if binry[i]==0 and y_actual[i]!=binry[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in lay data and analyze it with analyze_lay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path to where lay files are located\n",
    "path = r\"C:\\Users\\atill\\OneDrive - Johns Hopkins\\SarmaLab\\spike_detection\\data\"\n",
    "lay_files = glob.glob(path + \"/**/*.lay\", recursive = True)\n",
    "filename = [os.path.split(lay_files[i])[-1][:-4] for i in range(len(lay_files))]\n",
    "# Run analyze_lay to find spike data by patient and spike data by patient by channel \n",
    "spikedat = [analyze_lay(lay_files[j])[0] for j in range(len(lay_files))]\n",
    "spikechrates = [analyze_lay(lay_files[j])[1] for j in range(len(lay_files))]\n",
    "spikedict = dict(zip(filename, spikedat))\n",
    "# Append 'spikes' column (spikes / min / pt) to df\n",
    "df3 = numbidx(spikedict, df, 'spikes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1: Max spikes (maxspikech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-64bf7b6d2c16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspikechdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspikechrates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspikechdict2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspikechdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspikechdict2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Find max spike rate value in spikechdict and append to spikechdict2 (a dictionary containing maximum spike rate)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspikechdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "spikechdict = dict(zip(filename, spikechrates))\n",
    "spikechdict2 = spikechdict.copy()\n",
    "spikechdict2.keys()\n",
    "# Find max spike rate value in spikechdict and append to spikechdict2 (a dictionary containing maximum spike rate) \n",
    "for key, value in spikechdict.items():\n",
    "    if spikechdict2[key] == {}:\n",
    "        spikechdict2[key] = {'0':0}\n",
    "    spikechdict2[key] = max(spikechdict2[key].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract channels by electrode lobe location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate spike data in pt groups, append spikechdict data by temporal, frontal, occipital, and parietal lobes \n",
    "# into pt grouped dicts \n",
    "chdictnorm, chdictepnorm, chdictepabnorm = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "indnorm = df3[df3['patient_id']==0.0].hospital_id.values.tolist()\n",
    "indep0 = df3[df3['patient_id']==1.0].hospital_id.values.tolist()\n",
    "# Enclude these patients in the next line due to being outliers, having 0 spikes, etc criteria\n",
    "filter = [3,48,57,63] \n",
    "filtered_list = []\n",
    "indnormidx, indabnormidx, grps = [], [], []\n",
    "indep = [element for element in indep0 if element not in filter]\n",
    "indnormidx = [str(i) + '-clip' for i in indnorm]\n",
    "indabnormidx = [str(i) + '-clip' for i in indep0]\n",
    "grps = indnorm + indep0\n",
    "indnormep0 = [i for i in range(1,87) if i not in grps]\n",
    "indnormepidx = [str(i) + '-clip' for i in range(1,87) if i not in grps]\n",
    "for i in range(len(indnormidx)-1):\n",
    "    if indnorm[i] > 76:\n",
    "        indnormidx[i] = str(indnorm[i]) + 'awake-clip'\n",
    "        if indnorm[i] in [76, 79, 81, 83, 84]:\n",
    "            indnormidx.append(str(indnorm[i]) + 'sleep-clip')\n",
    "        elif indnorm[i] == 77:\n",
    "            indnormidx.append(str(indnorm[i]) + 'lightsleep-clip')\n",
    "for i in range(len(indabnormidx)-1):\n",
    "    if indep0[i] > 76:\n",
    "        indabnormidx[i] = str(indep0[i]) + 'awake-clip'\n",
    "        if indep0[i] in [76, 79, 81, 83, 84]:\n",
    "            indabnormidx.append(str(indep0[i]) + 'sleep-clip')\n",
    "for i in range(len(indnormepidx)-1):\n",
    "    if indnormep0[i] > 76:\n",
    "        indnormepidx[i] = str(indnormep0[i]) + 'awake-clip'\n",
    "        if indnormep0[i] in [76, 79, 81, 83, 84, 86]:\n",
    "            indnormepidx.append(str(indnormep0[i]) + 'sleep-clip')\n",
    "# Each pt group contains spike data grouped by first two characters in channel name if channel name begins \n",
    "# with t (temporal), f (frontal), o (occipital), or p (parietal)\n",
    "for i in spikechdict.keys():\n",
    "    for k in spikechdict[i].keys():\n",
    "        if k[0] in ['t', 'f', 'o', 'p']:\n",
    "            if i in indnormidx:\n",
    "                chdictnorm[k[0:2]].append(spikechdict[i][k])\n",
    "            elif i in indnormepidx:\n",
    "                chdictepnorm[k[0:2]].append(spikechdict[i][k])\n",
    "            elif i in indabnormidx:\n",
    "                chdictepabnorm[k[0:2]].append(spikechdict[i][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate lobe data (t, f, o, p)\n",
    "\n",
    "### tL, tR, meansrest (of brain) and readied to be appended to df as features in this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = dict(Counter(t3) + Counter(t5))\n",
    "meanstleft = {k: sums[k] / float((k in t3) + (k in t5)) for k in sums}\n",
    "sums2 = dict(Counter(t4) + Counter(t6))\n",
    "meanstrght = {k: sums2[k] / float((k in t4) + (k in t6)) for k in sums2}\n",
    "tR = t3.copy()\n",
    "tL = t4.copy()\n",
    "for key in meanstrght:\n",
    "    tR[key] = meanstrght[key]\n",
    "for key in meanstleft:\n",
    "    tL[key] = meanstleft[key]\n",
    "sums3 = dict(Counter(tL) + Counter(tR))\n",
    "meansall = {k: sums3[k] / float((k in tL) + (k in tR)) for k in sums3}\n",
    "tall = tR.copy()\n",
    "for key in meansall:\n",
    "    tall[key] = meansall[key]\n",
    "\n",
    "sums4 = dict(Counter(f) + Counter(o) + Counter(p))\n",
    "meansrest = {k: sums4[k] / float((k in f) + (k in o) + (k in p)) for k in sums4}\n",
    "meansrest2 = p.copy()\n",
    "for key in meansrest:\n",
    "    meansrest2[key] = meansrest[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot average spikes / min per channel per patient group\n",
    "\n",
    "## Adam put very well during the presentation that there are many more channels with the first starting character, and that the jutter that I added can make the plots harder to understand (e.g. overall t should see analogous spikes from t3, t4..etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in spikechdict.keys():\n",
    "    for k in spikechdict[i].keys():\n",
    "        if k[0] in ['t', 'f', 'o', 'p']:\n",
    "            if i in indnormidx:\n",
    "                chdictnorm[k[0:1]].append(spikechdict[i][k])\n",
    "            elif i in indnormepidx:\n",
    "                chdictepnorm[k[0:1]].append(spikechdict[i][k])\n",
    "            elif i in indabnormidx:\n",
    "                chdictepabnorm[k[0:1]].append(spikechdict[i][k])\n",
    "offst = 0.0 # This line can provide a slight random jitter for data visualization (0.015 was used for the presentation, but here it is 0)\n",
    "xn, yn = zip(*((float(x)+offst*random.randint(1,5), k) for k in sorted(chdictnorm) for x in chdictnorm[k]))\n",
    "xen, yen = zip(*((float(x)+offst*5*random.randint(1,5), k) for k in sorted(chdictepnorm) for x in chdictepnorm[k]))\n",
    "xabe, yabe = zip(*((float(x)+offst*5*random.randint(1,5), k) for k in sorted(chdictepabnorm) for x in chdictepabnorm[k]))\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].scatter(yn, xn, c=\"k\", alpha=1/2, s=90)\n",
    "axs[0].set_title('norm')\n",
    "plt.grid(linestyle='dotted')\n",
    "axs[1].scatter(yen, xen, c=\"c\", alpha=1/2, s=90)\n",
    "axs[1].set_title('ep-norm')\n",
    "axs[0].grid(linestyle='dotted')\n",
    "axs[1].grid(linestyle='dotted')\n",
    "axs[2].set_title('ep-abnorm')\n",
    "axs[2].scatter(yabe, xabe, c=\"m\", alpha=1/2, s=90)\n",
    "plt.suptitle('Average spikes / min per channel per grp')\n",
    "fig.text(0.5, 0.04, 'lobe', ha='center')\n",
    "fig.text(0.04, 0.5, 'avg spikes / min', va='center', rotation='vertical')\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.grid(linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign feature candidates (lobe data and others) to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = numbidx(spikechdict2, df3, 'maxspikech')\n",
    "df3 = numbidx(tall, df3, 't_lobespk')\n",
    "df3 = numbidx(tR, df3, 't_Rlobespk')\n",
    "df3 = numbidx(tL, df3, 't_Llobespk')\n",
    "df3 = numbidx(meansrest2, df3, 'f_o_p_lobespk')\n",
    "df3 = df3.fillna(0.0)\n",
    "#df3 = numbidx(o, df3, 'o_lobe') # meansrest can be split up in this manner (e.g. o can be separated in this way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of section of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide pt groups in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select equal sets of normal and abnormal (epileptic)\n",
    "rindnorm=[]\n",
    "indnorm = df3[df3['patient_id']==0.0].index.values.tolist()\n",
    "indnormep = df3[df3['patient_id']==0.5].index.values.tolist()\n",
    "indep0 = df3[df3['patient_id']==1.0].index.values.tolist()\n",
    "filter = [3,48,57,63]\n",
    "filtered_list = []\n",
    "indep = [element for element in indep0 if element not in filter]\n",
    "rindnorm = random.sample(indnorm, round(len(indep)/2))\n",
    "# Extend list of normal-nonepilepsy with normal-epilepsy\n",
    "# This was updated from the presentation on 8/9. \n",
    "rindnorm.extend(random.sample(indnormep, round(len(indep)/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 contains labels for normal-nonepilepsy=0, normal-epilepsy=0.5, abnormal-epilepsy=1.0\n",
    "df4 = df3.copy()\n",
    "# df3 further binarizes label for logistic regression as normal-nonepilepsy,normal-epilepsy=0, abnormal-epilepsy=1.0 \n",
    "# via filter\n",
    "filter = lambda x : 0 if x < 1 else 1\n",
    "df3['patient_id'] = df3.patient_id.apply(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 4 features via SelectKBest and chi2 method \n",
    "selector = SelectKBest(chi2, k = 4)\n",
    "x = df3[df3.columns[3:]].iloc[rindnorm + indep]\n",
    "y = df3[['patient_id']].iloc[rindnorm + indep]\n",
    "X_new = selector.fit_transform(x, y)\n",
    "names = x.columns.values[selector.get_support()]\n",
    "scores = selector.scores_[selector.get_support()]\n",
    "pvalues = selector.pvalues_[selector.get_support()]\n",
    "names_scores = list(zip(names, pvalues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names)\n",
    "print(scores)\n",
    "print(names_scores)\n",
    "print(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish x (features) and y (label) vectors\n",
    "x = df3[names].iloc[rindnorm + indep]\n",
    "y = df3[['patient_id']].iloc[rindnorm + indep] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stratifiedshuffle split on x, y data for train, test groups\n",
    "xysplit = StratifiedShuffleSplit(n_splits=10, test_size=0.2) # change n_splits for n groups to test\n",
    "xysplit.get_n_splits(x,y)\n",
    "x_train, x_test, y_train, y_test = [], [], [], []\n",
    "for train_index, test_index in xysplit.split(x, y):\n",
    "    x_train.append(x[names].iloc[train_index])\n",
    "    x_test.append(x[names].iloc[test_index])\n",
    "    y_train.append(y[['patient_id']].iloc[train_index])\n",
    "    y_test.append(y[['patient_id']].iloc[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep an example test, train grouping using below block\n",
    "\n",
    "### model perfomance varies greatly depending on the luck of the train/test group selected at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = x_train\n",
    "x_test2 = x_test\n",
    "y_train2 = y_train\n",
    "y_test2 = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run logistic regression\n",
    "\n",
    "## Apply sm.Logit with a constant and no penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [0]*len(y_test)\n",
    "for i in range(len(y_test)):\n",
    "    # Modify features to use via manipulating x_train[i], (e.g. max_spikes only is x_train[i].max_spikes)\n",
    "    model = sm.Logit(y_train[i], sm.add_constant(x_train[i].astype(float)),penalty=None).fit()\n",
    "    print(model.summary()) # Can comment this line if model summary not desired\n",
    "    y_pred[i] = model.predict(exog=sm.add_constant(x_test[i].astype(float)))\n",
    "    print(i, model.params) # Can comment this and the next line if model parameter display not desired\n",
    "    print(i, model.pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe feature characteristics as a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = df4.boxplot(column=names.tolist(), by='patient_id', grid=False)\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = df3.boxplot(column=names.tolist(), by='patient_id', grid=False)\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add predicted values to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.DataFrame(df3.patient_id, df3.hospital_id)\n",
    "dfp = dfp.reset_index()\n",
    "dfp['p'] = np.NaN\n",
    "for i in range(len(y_pred)):    \n",
    "    dfp['p'].update(dfp['hospital_id'].map(y_pred[i].to_dict()))\n",
    "dfp = dfp.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize feature by patient group (feel free to change y= to another value)\n",
    "\n",
    "### Colorcoding by epilepsy group (0.0, 0.5) not incorporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"patient_id\", y=\"f_o_p_lobespk\", data=df4, showfliers = False, color='white', width=0.25)\n",
    "ax = sns.swarmplot(x=\"patient_id\", y=\"f_o_p_lobespk\", data=df4, color=\".25\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show ROC curves\n",
    "\n",
    "### Confidence interval visualization for ROCs not incorporated yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs, fprs, tprs, threshs = list(), list(), list(), list()\n",
    "ax = plt.axes()\n",
    "for i in range(len(y_test)):\n",
    "    fpr, tpr, thresh = roc_curve(y_test[i], y_pred[i])\n",
    "    fprs.append(fpr), tprs.append(tpr), threshs.append(thresh)\n",
    "    aucx = roc_auc_score(y_test[i], y_pred[i])\n",
    "    aucs.append(aucx)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='LogReg')\n",
    "    plt.xlabel('False Pos Rate')\n",
    "    plt.ylabel('True Pos Rate')\n",
    "    plt.title('LogReg ROC Curve for Normal/Abnormal-Epilepsy Groups')\n",
    "    plt.rcParams['figure.figsize'] = [6, 5]\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal threshold via argmax of tprs - fprs and max accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acclist, optimal_threshs = list(), list()\n",
    "for i in range(len(fprs)):\n",
    "    optimal_idx = np.argmax(tprs[i] - fprs[i])\n",
    "    optimal_threshs.append(threshs[i][optimal_idx])\n",
    "    TP, FP, TN, FN = perf_measure(dfp.patient_id, dfp.p, optimal_threshs[i])\n",
    "    Acclist.append((TP + TN)/ (TP + FP + TN + FN))\n",
    "idx = Acclist.index(max(Acclist))\n",
    "optimal_threshold = optimal_threshs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report accuracy, sensitivity, and specificity of model on tested data\n",
    "\n",
    "### Accuracy has diminished since the addition of normal-epilepsy groups, unfortunately --\n",
    "\n",
    "### Incorporation of spectral data, artifact removal, and/or feature revision may address this\n",
    "\n",
    "#### Modification of 'rindnorm' group to exclude normal-epilepsy patients returns what was seen in the presentation, with luck of the train/test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, TN, FN = perf_measure(dfp.patient_id, dfp.p, optimal_threshold)\n",
    "Acc = (TP + TN)/ (TP + FP + TN + FN)\n",
    "Sens = TP / (TP + FN)\n",
    "Spec = TN / (TN + FP)\n",
    "print('The accuracy of tested data is ', Acc, '. The sensitivity of tested data is ', Sens, '. The specificity of tested data is ', Spec, '.', 'Mean AUC is', np.mean(aucs), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize prob(epilepsy) by patient group\n",
    "### Colorcoding by epilepsy group (0.0 and 0.5) not incorporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.boxplot(column='p', by='patient_id', grid=False)\n",
    "ax = sns.swarmplot(x=\"patient_id\", y=\"p\", data=dfp, color=\".25\") \n",
    "plt.ylabel('P(epilepsy)');\n",
    "plt.plot([-1, 2], [optimal_threshold, optimal_threshold], 'm--')\n",
    "print('Optimal theta calculated as', optimal_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "episcalp",
   "language": "python",
   "name": "episcalp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
