{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mne_bids import get_entities_from_fname\n",
    "from natsort import natsorted\n",
    "#from rerf.rerfClassifier import rerfClassifier\n",
    "\n",
    "# comparative classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.metaestimators import _safe_split\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    brier_score_loss,\n",
    "    plot_precision_recall_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    cross_validate,\n",
    "    GroupShuffleSplit,\n",
    "    StratifiedShuffleSplit\n",
    ")\n",
    "\n",
    "#import dabest\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import feature loading code\n",
    "from sample_code.study import (\n",
    "    load_patient_dict,\n",
    "    determine_feature_importances,\n",
    "    tune_hyperparameters,\n",
    "    # extract_Xy_pairs,\n",
    "    # _sequential_aggregation,\n",
    "    format_supervised_dataset,\n",
    ")\n",
    "\n",
    "from sample_code.io import read_participants_tsv, load_feature_data\n",
    "from sample_code.utils import _load_turbo, _plot_roc_curve, NumpyEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_subject_groups = {\n",
    "    'non-epilepsy': 0,\n",
    "    'epilepsy-normal': 1,\n",
    "}\n",
    "include_feature_groups = ['sourcesink', 'fragility']\n",
    "cross_val_splits = 10\n",
    "classify_abnormal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_roc(fpr, tpr):\n",
    "    \"\"\"Compute average ROC statistics.\"\"\"\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 200)\n",
    "    \n",
    "    n_splits = len(fpr)\n",
    "    print(f\"Computing average ROC over {n_splits} CV splits\")\n",
    "    for i in range(n_splits):\n",
    "        interp_tpr = np.interp(mean_fpr, fpr[i], tpr[i])\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(auc(mean_fpr, interp_tpr))\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    return mean_fpr, tprs, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_patient_predictions(\n",
    "    y_trues, ypred_probs, subject_groups, pat_predictions=None, pat_true=None\n",
    "):\n",
    "    if pat_predictions is None or pat_true is None:\n",
    "        pat_predictions = collections.defaultdict(list)\n",
    "        pat_true = dict()\n",
    "        \n",
    "    # loop through things\n",
    "    for ytrue, ypred_proba, subject in zip(y_trues, ypred_probs, subject_groups):\n",
    "        pat_predictions[str(subject)].append(float(ypred_proba))\n",
    "        \n",
    "        if subject not in pat_true:\n",
    "            pat_true[str(subject)] = ytrue\n",
    "        else:\n",
    "            if pat_true[str(subject)] != ytrue:\n",
    "                raise RuntimeError(\"subjects should match...\")\n",
    "    return pat_predictions, pat_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bids_root = Path(\"...\")\\nderiv_path = bids_root / \"derivatives\"\\nsource_path = bids_root = \"sourcedata\"\\n\\nexcel_fpath = source_path / \"...\"\\n\\nintermed_fpath = path(deriv_path) / \"baselinesliced\"\\n\\n# where to save results\\nstudy_path = Path(deriv_path) / \"study\"\\n\\n# feature names\\nfeature_names = [\\n    \"kl_div\"\\n]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 12345\n",
    "random_state = 12345\n",
    "np.random.seed(seed)\n",
    "\n",
    "# proportion of subjects used for training\n",
    "train_size = 0.6\n",
    "\n",
    "# classification model to use\n",
    "clf_type = \"mtmorf\"\n",
    "\n",
    "# BIDS related directories\n",
    "\"\"\"bids_root = Path(\"...\")\n",
    "deriv_path = bids_root / \"derivatives\"\n",
    "source_path = bids_root = \"sourcedata\"\n",
    "\n",
    "excel_fpath = source_path / \"...\"\n",
    "\n",
    "intermed_fpath = path(deriv_path) / \"baselinesliced\"\n",
    "\n",
    "# where to save results\n",
    "study_path = Path(deriv_path) / \"study\"\n",
    "\n",
    "# feature names\n",
    "feature_names = [\n",
    "    \"kl_div\"\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining evaluation criterion\n",
    "metric = \"roc_auc\"\n",
    "BOOTSTRAP = False\n",
    "\n",
    "# defining hyperparameters\n",
    "\n",
    "ncores = -1\n",
    "num_runs = 1\n",
    "n_est = 500  # number of estimators\n",
    "\n",
    "\n",
    "names = {\n",
    "    \"Log. Reg\": \"blue\",\n",
    "    #\"Lin. SVM\": \"firebrick\",\n",
    "}\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=0, n_jobs=ncores, solver=\"liblinear\"),\n",
    "    #LinearSVC(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    feature_name,\n",
    "    include_groups,\n",
    "    deriv_path,\n",
    "    excel_fpath,\n",
    "    feature_names,\n",
    "    json_fpath,\n",
    "    patient_aggregation_method=None,\n",
    "    intermed_fpath=None,\n",
    "    save_cv_indices: bool=False,\n",
    "):\n",
    "    if feature_name == \"sourcesink\":\n",
    "        if not intermed_fpath:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                subjects,\n",
    "                ch_names,\n",
    "                centers\n",
    "            ) = load_feature_data(\"sourcesink\", \"npy\", deriv_path, excel_fpath=excel_fpath, feature_names=feature_names, json_fpath=json_fpath, include_groups=include_groups)\n",
    "        else:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                subjects,\n",
    "                ch_names,\n",
    "                centers\n",
    "            ) = load_feature_data(\"sourcesink\", \"npy\",intermed_fpath, excel_fpath=excel_fpath, feature_names=feature_names, json_fpath=json_fpath, include_groups=include_groups)\n",
    "    elif feature_name == \"spikes\":\n",
    "        if not intermed_fpath:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                subjects,\n",
    "                ch_names,\n",
    "                centers\n",
    "            ) = load_feature_data(\"spikes\", \"json\", deriv_path, excel_fpath=excel_fpath, feature_names=feature_names, json_fpath=json_fpath, include_groups=include_groups)\n",
    "        else:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                subjects,\n",
    "                ch_names,\n",
    "                centers\n",
    "            ) = load_feature_data(\"spikes\", \"json\",intermed_fpath, excel_fpath=excel_fpath, feature_names=feature_names, json_fpath=json_fpath, include_groups=include_groups)\n",
    "    elif feature_name == \"fragility\":\n",
    "        if not intermed_fpath:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                subjects,\n",
    "                ch_names,\n",
    "                centers\n",
    "            ) = load_feature_data(\"fragility\", \"npy\", deriv_path, excel_fpath=excel_fpath, feature_names=feature_names, json_fpath=json_fpath, include_groups=include_groups)\n",
    "        else:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                subjects,\n",
    "                ch_names,\n",
    "                centers\n",
    "            ) = load_feature_data(\"fragility\", \"npy\",intermed_fpath, excel_fpath=excel_fpath, feature_names=feature_names, json_fpath=json_fpath, include_groups=include_groups)\n",
    "    else:\n",
    "        print(f\"{feature_name} is unknown\")\n",
    "        return None\n",
    "    subject_groups = [int(g) for g in subject_groups]\n",
    "    subject_groups = np.array(subject_groups)\n",
    "\n",
    "    # create held-out test dataset\n",
    "    # create separate pool of subjects for testing dataset\n",
    "    # 1. Cross Validation Training / Testing Split\n",
    "    study_path = deriv_path / \"study\"\n",
    "    if save_cv_indices:\n",
    "        sss = StratifiedShuffleSplit(n_splits=10, train_size=0.5, random_state=random_state)\n",
    "        for jdx, (train_inds, test_inds) in enumerate(\n",
    "            sss.split(unformatted_X, y)\n",
    "        ):\n",
    "            # if jdx != 7:\n",
    "            #     continue\n",
    "            train_pats = np.unique(subject_groups[train_inds])\n",
    "            test_pats = np.unique(subject_groups[test_inds])\n",
    "            save_fpath = study_path / \"inds\" / \"fixed_folds_subjects\" / f\"{feature_name}-srerf-{jdx}-inds.npz\"\n",
    "            Path(save_fpath.parent).mkdir(parents=True, exist_ok=True)\n",
    "            np.savez_compressed(\n",
    "                save_fpath,\n",
    "                train_inds=train_inds,\n",
    "                test_inds=test_inds,\n",
    "                train_pats=train_pats,\n",
    "                test_pats=test_pats,\n",
    "            )\n",
    "    return unformatted_X, y, subject_groups, subjects, ch_names, centers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clf_validation(\n",
    "    clf_type,\n",
    "    clf_func,\n",
    "    unformatted_X,\n",
    "    y,\n",
    "    subject_groups,\n",
    "    study_path,\n",
    "    feature_name='sourcesink'\n",
    "):\n",
    "    unformatted_X = unformatted_X.copy()\n",
    "    y = y.copy()\n",
    "    subject_groups = subject_groups.copy()\n",
    "    \n",
    "    for jdx in range(1, 10):\n",
    "        cv_scores = collections.defaultdict(list)\n",
    "    \n",
    "        \n",
    "        with np.load(\n",
    "            # study_path / \"inds\" / 'clinical_complexity' / f\"{jdx}-inds.npz\",\n",
    "            study_path\n",
    "            / \"inds\"\n",
    "            / \"fixed_folds_subjects\"\n",
    "            / f\"sourcesink-srerf-{jdx}-inds.npz\",\n",
    "            allow_pickle=True,\n",
    "        ) as data_dict:\n",
    "            # train_inds, test_inds = data_dict[\"train_inds\"], data_dict[\"test_inds\"]\n",
    "            train_pats, test_pats = data_dict[\"train_pats\"], data_dict[\"test_pats\"]\n",
    "\n",
    "        # set train indices based on which subjects\n",
    "        train_inds = [\n",
    "            idx for idx, sub in enumerate(subject_groups) if sub in train_pats\n",
    "        ]\n",
    "        test_inds = [idx for idx, sub in enumerate(subject_groups) if sub in test_pats]\n",
    "        \n",
    "        subjects_test = subject_groups[test_inds]\n",
    "        \n",
    "        \n",
    "        X_formatted, dropped_inds = format_supervised_dataset(\n",
    "            unformatted_X\n",
    "        )\n",
    "        \n",
    "        clf = clf_func\n",
    "        print(\"Updated classifier: \", clf)\n",
    "\n",
    "        # perform CV using Sklearn\n",
    "        scoring_funcs = {\n",
    "            \"roc_auc\": roc_auc_score,\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"balanced_accuracy\": balanced_accuracy_score,\n",
    "            \"average_precision\": average_precision_score,\n",
    "        }\n",
    "\n",
    "        def dummy_cv(train, test):\n",
    "            yield train_inds, test_inds\n",
    "\n",
    "        n_samps = len(y)\n",
    "        if isinstance(clf, KerasClassifier):\n",
    "            print(X_formatted.shape)\n",
    "            X_formatted = X_formatted.reshape(n_samps, 20, np.sum(window), 1)\n",
    "            print(\"new shape: \", X_formatted.shape)\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        \n",
    "        cv = dummy_cv(train_inds, test_inds)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            clf,\n",
    "            X_formatted,\n",
    "            y,\n",
    "            groups=subject_groups,\n",
    "            cv=cv,\n",
    "            scoring=list(scoring_funcs.keys()),\n",
    "            return_estimator=True,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        # get the best classifier based on pre-chosen metric\n",
    "        test_key = f\"test_{metric}\"\n",
    "\n",
    "        # removing array like structure\n",
    "        scores = {key: val[0] for key, val in scores.items()}\n",
    "        estimator = scores.pop(\"estimator\")\n",
    "        \n",
    "        coeff_ = estimator.coeff_\n",
    "        intercept_ = estimator.intercept_\n",
    "        coeff = intercept_.copy()\n",
    "        [coeff.extend(c) for c in coeff]\n",
    "        \n",
    "        X_test, y_test = np.array(X_formatted)[test_inds, ...], np.array(y)[test_inds]\n",
    "        groups_test = np.array(subject_groups)[test_inds]\n",
    "        \n",
    "        \n",
    "        y_pred_prob = estimator.predict_proba(X_formatted)[:, 1]\n",
    "        y_pred = estimator.predict(X_formatted)\n",
    "        \n",
    "        cv_scores[\"validate_ytrue\"].append(list(y_test))\n",
    "        cv_scores[\"validate_ypred_prob\"].append(list(y_pred_prob))\n",
    "        cv_scores[\"validate_ypred\"].append(list(y_pred))\n",
    "        cv_scores[\"validate_subject_groups\"].append(list(groups_test))\n",
    "\n",
    "        # store ROC curve metrics on the held-out test set\n",
    "        fpr, tpr, thresholds = roc_curve(y, y_pred_prob, pos_label=1)\n",
    "        fnr, tnr, neg_thresholds = roc_curve(y, y_pred_prob, pos_label=0)\n",
    "        cv_scores[\"validate_fpr\"].append(list(fpr))\n",
    "        cv_scores[\"validate_tpr\"].append(list(tpr))\n",
    "        cv_scores[\"validate_fnr\"].append(list(fnr))\n",
    "        cv_scores[\"validate_tnr\"].append(list(tnr))\n",
    "        cv_scores[\"validate_thresholds\"].append(list(thresholds))\n",
    "        cv_scores[\"coeff\"].append(list(coeff))\n",
    "        \n",
    "        try:\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y, y_pred_prob, n_bins=10, strategy=\"quantile\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                print(e)\n",
    "                fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                    y, y_pred_prob, n_bins=5, strategy=\"uniform\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                fraction_of_positives = [None]\n",
    "                mean_predicted_value = [None]\n",
    "        clf_brier_score = np.round(\n",
    "            brier_score_loss(y, y_pred_prob, pos_label=np.array(y).max()), 2\n",
    "        )\n",
    "        \n",
    "        print(\"Done analyzing calibration stats...\")\n",
    "\n",
    "        # store ingredients for a calibration curve\n",
    "        cv_scores[\"validate_brier_score\"].append(float(clf_brier_score))\n",
    "        cv_scores[\"validate_fraction_pos\"].append(list(fraction_of_positives))\n",
    "        cv_scores[\"validate_mean_pred_value\"].append(list(mean_predicted_value))\n",
    "\n",
    "        pat_predictions, pat_true = combine_patient_predictions(\n",
    "            y, y_pred_prob, subjects_test\n",
    "        )\n",
    "        cv_scores[\"validate_pat_predictions\"].append(pat_predictions)\n",
    "        cv_scores[\"validate_pat_true\"].append(pat_true)\n",
    "        \n",
    "        # store output for feature importances\n",
    "        if clf_type == \"rf\":\n",
    "            n_jobs = -1\n",
    "        else:\n",
    "            n_jobs = 1\n",
    "\n",
    "        if not isinstance(clf, KerasClassifier):\n",
    "            results = determine_feature_importances(\n",
    "                estimator, X_formatted, y, n_jobs=n_jobs\n",
    "            )\n",
    "            imp_std = results.importances_std\n",
    "            imp_vals = results.importances_mean\n",
    "            cv_scores[\"validate_imp_mean\"].append(list(imp_vals))\n",
    "            cv_scores[\"validate_imp_std\"].append(list(imp_std))\n",
    "\n",
    "            print(\"Done analyzing feature importances...\")\n",
    "\n",
    "        # save intermediate analyses\n",
    "        clf_func_path = (\n",
    "            study_path\n",
    "            / \"clf-train-vs-test\"\n",
    "            / \"classifiers\"\n",
    "            / f\"{clf_type}_classifiers_{feature_name}_{jdx}.npz\"\n",
    "        )\n",
    "        clf_func_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # nested CV scores\n",
    "        nested_scores_fpath = (\n",
    "            study_path\n",
    "            / \"clf-train-vs-test\"\n",
    "            / f\"study_cv_scores_{clf_type}_{feature_name}_{jdx}.json\"\n",
    "        )\n",
    "\n",
    "        # save the estimators\n",
    "        if clf_type not in [\"srerf\", \"mtmorf\"]:\n",
    "            np.savez_compressed(clf_func_path, estimators=estimator)\n",
    "\n",
    "        # save all the master scores as a JSON file\n",
    "        with open(nested_scores_fpath, \"w+\") as fin:\n",
    "            json.dump({str(k): cv_scores[k] for k in cv_scores}, fin, cls=NumpyEncoder)\n",
    "            #json.dump(cv_scores, fin, cls=NumpyEncoder)\n",
    "\n",
    "        del estimator\n",
    "        del scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = {\n",
    "    \"sourcesink\": [\"kldiv\", \"entropy\", \"variance\", \"skew\", \"kurtosis\"],\n",
    "    \"spikes\": [\"spike_rate\", \"max_spikes\"],\n",
    "    \"fragility\": [\"kldiv\", \"entropy\", \"variance\", \"skew\", \"kurtosis\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_path = Path(\"D:/OneDriveParent/OneDrive - Johns Hopkins/Shared Documents/40Hz-30/derivatives\")\n",
    "excel_fpath = Path(\"D:/ScalpData/JHU_scalp_clinical_datasheet_raw_local.xlsx\")\n",
    "#feature_names = [\"kldiv\", \"entropy\", \"variance\", \"skew\", \"kurtosis\"]\n",
    "#spike_feature_names = [\"spike_rate\", \"max_spikes\"]\n",
    "json_fpath = Path(\"D:/Desktop/ezscalp/scripts/epilepsy_inds_single.json\")\n",
    "study_path = deriv_path / \"study\"\n",
    "\n",
    "unformatted_X = []\n",
    "for ind, feature_group in enumerate(include_feature_groups):\n",
    "    part_X, y, subject_groups, subjects, ch_names, centers = load_data(\n",
    "        feature_group,\n",
    "        include_subject_groups,\n",
    "        deriv_path,\n",
    "        excel_fpath,\n",
    "        feature_names[feature_group],\n",
    "        json_fpath,\n",
    "        save_cv_indices=True\n",
    "    )\n",
    "    if ind == 0:\n",
    "        unformatted_X_ = part_X\n",
    "    else:\n",
    "        unformatted_X_ = [x+p for x,p in zip(unformatted_X, part_X)]\n",
    "    unformatted_X = unformatted_X_.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_feature_names = [\"ss-kldiv\", \"ss-entropy\", \"ss-variance\", \"ss-skew\", \"ss-kurtosis\", \"spike_rate\", \"max_spikes\", \"frag-kldiv\", \"frag-entropy\", \"frag-variance\", \"frag-skew\", \"frag-kurtosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done analyzing calibration stats...\n",
      "Done analyzing feature importances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\.virtualenvs\\episcalp-235KLvvi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf_func in zip(names, classifiers):\n",
    "    run_clf_validation(\n",
    "        clf_name,\n",
    "        clf_func,\n",
    "        unformatted_X,\n",
    "        y,\n",
    "        subject_groups,\n",
    "        study_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_cv(train, test):\n",
    "    yield train_inds, test_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if==0:\n",
    "    jdx = 8\n",
    "    with np.load(\n",
    "        # study_path / \"inds\" / 'clinical_complexity' / f\"{jdx}-inds.npz\",\n",
    "        study_path\n",
    "        / \"inds\"\n",
    "        / \"fixed_folds_subjects\"\n",
    "        / f\"sourcesink-srerf-{jdx}-inds.npz\",\n",
    "        allow_pickle=True,\n",
    "    ) as data_dict:\n",
    "        # train_inds, test_inds = data_dict[\"train_inds\"], data_dict[\"test_inds\"]\n",
    "        train_pats, test_pats = data_dict[\"train_pats\"], data_dict[\"test_pats\"]\n",
    "\n",
    "    # set train indices based on which subjects\n",
    "    train_inds = [\n",
    "        idx for idx, sub in enumerate(subject_groups) if sub in train_pats\n",
    "    ]\n",
    "    test_inds = [idx for idx, sub in enumerate(subject_groups) if sub in test_pats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if==0:\n",
    "    X, dropped_inds = format_supervised_dataset(\n",
    "                unformatted_X\n",
    "            )\n",
    "    clf = LogisticRegression(random_state=0, n_jobs=ncores, solver=\"liblinear\")\n",
    "    X_train, y_train = _safe_split(clf, X, y, train_inds)\n",
    "    X_test, y_test = _safe_split(clf, X, y, test_inds, train_inds)\n",
    "    clf.fit(X_formatted, y)\n",
    "    clf.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "episcalp",
   "language": "python",
   "name": "episcalp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
